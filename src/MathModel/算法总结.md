---
title: 算法总结
tag:
  - 数学建模
date: 2024-08-08
cover: https://cdn.jsdelivr.net/gh/czlifetime/img/f557269f0a7a85cb5bb24a48d1f2542d.jpeg
---

数学建模算法总结

<!--more-->

算法步骤:

:::danger
1. 算法的数学原理、思想
2. 数学的计算过程
3. 计算机执行步骤
4. 代码

:::



## 运筹优化类

+ **线性规划**
+ **非线性规划**
    - **BFGS 算法:**求解**无约束**优化问题



:::info
BFGS算法试图通过近似Hessian矩阵（目标函数的二阶导数矩阵）来加速收敛，但并不直接计算Hessian矩阵，而是使用过去几次迭代的信息来更新一个近似矩阵。

### BFGS算法的主要特点：
1. **无需计算Hessian矩阵**：相比于牛顿法，BFGS不需要显式计算Hessian矩阵及其逆矩阵，这在大规模问题中是非常有利的，因为直接计算Hessian矩阵及其逆矩阵可能是非常昂贵的。
2. **全局收敛性**：在满足某些条件下，BFGS算法可以保证全局收敛至局部极小点。
3. **局部超线性收敛**：在靠近极小点的地方，BFGS算法可以达到超线性甚至二次收敛。

### BFGS算法的基本步骤：

1. **初始化**：选择初始点 x0，并初始化一个正定矩阵 B0（通常初始化为单位矩阵）作为Hessian矩阵的近似。
2. **计算搜索方向**：在第 k次迭代时，计算搜索方向 pk为$p_k=-B_k^{-1}∇f(x_k)$，其中$∇f(x_k)$是目标函数在$x_k$处的梯度。
3. **步长搜索**：使用某种一维搜索方法（如Armijo规则或Wolfe条件）来确定步长$α_k$，使得新的点$x_{k+1}$沿着搜索方向移动：$x_{x+1}=x_k+\alpha_kp_k$
4. **更新近似Hessian矩阵**：根据$x_k$和$x_{k+1}$的变化来更新$B_k$的近似值$B_{k+1}$。更新公式如下：$s_k=x_{k+1}-x_ky_k=∇f(x_{k+1})-∇f(x_k)B_{k+1}=B_k+\frac{y_ky_k^T}{y_k^Ts_k}-\frac{B_ks_ks_k^TB_k^T}{s_k^TB_ks_k}$
5. **终止条件**：检查是否达到终止条件（如梯度足够小或达到最大迭代次数）。如果没有达到，则转到步骤2。

### BFGS算法的优点:

+ **高效性**：BFGS算法通常比简单的梯度下降法收敛得更快。
+ **灵活性**：BFGS算法不需要直接计算Hessian矩阵，因此对于大规模问题特别有用。
+ **稳定性**：通过保持Hessian矩阵的正定性，BFGS算法避免了某些可能导致不稳定的情况。

:::

**L-BFGS-B 算法**

:::info
L-BFGS-B（Limited-memory Broyden-Fletcher-Goldfarb-Shanno Bound）算法是BFGS算法的一个变种，它特别适用于大规模优化问题，并且可以处理带有边界约束的问题。L-BFGS-B算法通过维护一个有限的历史信息来近似Hessian矩阵，而不是存储完整的Hessian矩阵或其逆矩阵，这使得它在内存使用方面更加高效。此外，L-BFGS-B允许指定变量的上下界，这使得它非常适合处理带有简单边界约束的优化问题。

### L-BFGS-B算法的特点：
1. **有限内存**：L-BFGS-B只保留最近几次迭代的信息来更新Hessian矩阵的近似值，这大大减少了内存消耗。通常，只需要保存5到10次迭代的信息即可获得良好的近似效果。
2. **边界约束**：L-BFGS-B允许指定每个变量的上下界，这样可以防止优化过程中的变量超出合理的范围。
3. **高效性**：由于其有限内存特性和对Hessian矩阵的高效近似，L-BFGS-B非常适合处理大规模优化问题。

### L-BFGS-B算法的基本步骤：
1. **初始化**：选择初始点$x_0$，并初始化一个正定矩阵$H_0$（通常初始化为单位矩阵）作为Hessian矩阵逆的近似。
2. **计算搜索方向**：在第 _k_ 次迭代时，计算搜索方向 _pk_ 为$p_k=-H_k∇f(x_k)$其中$∇F(x_k)$是目标函数在$x_k$处的梯度。
3. **步长搜索**：使用某种一维搜索方法（如Armijo规则或Wolfe条件）来确定步长$\alpha_k$，使得新的点$x_{k+1}$沿着搜索方向移动，并且满足边界约束：$x_{k+1}=x_k+α_kp_k$__
4. **更新近似Hessian矩阵逆**：根据$x_k$和$x_{k+1}$的变化来更新$H_k$的近似值$H_{k+1}$。更新公式类似于BFGS算法，但只保留最近几次迭代的信息。
5. **终止条件**：检查是否达到终止条件（如梯度足够小或达到最大迭代次数）。如果没有达到，则转到步骤2。

:::

+ **整数规划**
    - **分支定界法**

:::info
分支定界法是一种用于解决整数线性规划问题的算法。其基本思想是通过逐步缩小可行解的范围来找到最优解。

### 基本步骤
1. **初始化**：首先，解决不考虑整数约束的线性规划问题（称为松弛问题）。如果松弛问题的最优解满足整数约束，那么这个解就是原整数规划问题的最优解。
2. **分支**：如果松弛问题的最优解不满足整数约束，算法将选择一个不满足整数约束的变量，并创建两个子问题。每个子问题都增加了一个额外的约束，限制了该变量的取值范围（例如，一个子问题要求该变量小于或等于其松弛问题解的整数部分，另一个子问题要求该变量大于或等于其松弛问题解的整数部分加一）。
3. **定界**：对每个子问题重复步骤1和2，直到找到满足所有整数约束的最优解或证明某个子问题没有可行解。在搜索过程中，如果一个子问题的最优目标值已经超过了当前找到的最优整数解的目标值，那么这个子问题及其所有分支都可以被剪枝（即不再考虑）。
4. **剪枝**：如果一个子问题的解的目标值比当前找到的最优整数解还要差，那么这个子问题就没有进一步考虑的必要了。
5. **选择分支**：在选择分支时，通常会根据某种规则（如最违反整数约束的变量、最大残差等）来决定。
6. **终止条件**：当所有的分支都被探索完毕，或者所有的活跃分支都被剪枝，算法结束。

### 特点
+ **全局优化**：分支定界法可以找到全局最优解，而不是局部最优解。
+ **效率**：通过剪枝技术，分支定界法可以避免搜索不可能产生最优解的解空间区域。
+ **适用性**：适用于整数线性规划问题，特别是当问题规模不是非常大时。

### 限制
+ **计算复杂度**：对于某些问题，分支定界法的计算量可能非常大，特别是当问题的变量非常多时。
+ **内存需求**：在搜索过程中可能需要存储大量的子问题，这可能导致较高的内存需求。

:::

    - **割平面法**
    - **分支切割法**
    - **隐枚举法**
    - **混合整数规划**
+ **图论**
    - **最短路径**

:::info
**DFS 与 BFS**

**DFS**

深度优先搜索是一种沿着图的深度前进的算法，它首先访问一个起始节点，然后尽可能深入地探索每条分支。当到达一个节点的末端（即没有未访问过的邻居）时，算法会回溯到上一个节点，并继续探索其他未访问的分支。

**BFS**

广度优先搜索是一种按照层次遍历图的算法。它从一个起始节点开始，先访问所有直接相连的节点，然后再访问下一层的节点，依此类推，直到所有的节点都被访问。

:::

    - **最短生成树**
+ **启发式算法**
    - **模拟退火**

:::info
模拟退火算法（Simulated Annealing，简称SA）是一种启发式全局优化算法，灵感来源于固体冷却过程中的退火现象。在金属加工中，退火是指将金属加热到高温后缓慢冷却，使金属内部的原子达到较低的能量状态，从而减少内部应力，改善材料性能。类似地，模拟退火算法用于在优化问题中寻找全局最优解或近似最优解。

### 模拟退火算法的基本原理
模拟退火算法试图避免局部最优解，通过引入随机性来探索解空间，从而有可能跳出局部极小值，最终收敛到全局最优解或接近全局最优解的解。

### 模拟退火算法的主要步骤
1. **初始化**：
    - 选择一个初始解 x0__。
    - 设定初始温度 T0_T_0，降温系数 α_α_（通常 0<α<1），终止温度 Tend
2. **迭代过程**：
    - 在当前温度_T_下，生成一个新的候选解 x′__。
    - 计算新的候选解与当前解之间的目标函数差 ΔE=E(x′)−E(x)
    - 如果 ΔE<0，接受新的解 x′作为当前解 x。
    - 如果 ΔE>0，以一定概率接受新的解 x′作为当前解 x，该概率由 Metropolis 准则给出：P(accept x′)=e−ΔET,降低温度 T←αT。
    - 重复上述过程，直到温度降到终止温度 Tend。

### 关键组件
1. **温度控制**：温度 T_T_ 控制着算法接受劣解的概率。随着温度逐渐降低，接受劣解的概率也随之减小，从而逐渐趋近于局部最优解。
2. **冷却计划**：冷却计划决定了温度如何随时间变化。通常采用线性或指数冷却计划。
3. **邻域结构**：定义了如何从当前解生成新的候选解。邻域结构的选择影响算法的探索能力和收敛速度。

### 优点
+ **全局搜索能力**：通过允许一定程度的上坡移动，模拟退火算法能够在搜索过程中跳出局部极小值，从而有较高的概率找到全局最优解。
+ **鲁棒性**：算法对初始解不敏感，适用于广泛的优化问题。

### 缺点
+ **计算开销**：由于需要多次迭代，并且在早期阶段接受劣解的概率较高，算法的计算成本可能较高。
+ **参数选择**：初始温度、冷却系数和终止温度等参数的选择对算法的性能有很大影响，需要仔细调整。

### 应用场景
模拟退火算法广泛应用于组合优化问题、机器学习、神经网络训练、路径规划等问题中。例如：

+ **旅行商问题（TSP）**：寻找访问一系列城市并返回起点的最短路径。
+ **电路板布局设计**：优化电子元件在电路板上的布局。
+ **特征选择**：在机器学习中选择最优特征子集。
+ **调度问题**：优化生产流程或资源分配。

:::

    - **遗传算法**

:::info
遗传算法（Genetic Algorithm, GA）是一种基于生物进化原理的全局优化搜索算法。它模仿自然界中的自然选择和遗传机制，通过“适者生存”的原则，在解空间中搜索最优或近似最优解。遗传算法是进化算法的一种，适用于解决各种类型的问题，包括但不限于连续函数优化、组合优化、约束满足问题等。

### 遗传算法的基本思想
遗传算法受到达尔文的自然选择理论启发，其核心思想是在一个种群中通过选择、交叉（杂交）、变异等操作产生下一代个体，并且这一过程不断重复，直到找到满意解或达到预定的停止条件为止。

### 主要组成部分
1. **编码（Representation）**：将问题的解表示成染色体（Chromosome），通常是一串二进制位或其他形式的数据结构。
2. **适应度函数（Fitness Function）**：评价个体的好坏程度，即解的质量。适应度函数的设计直接影响算法的性能。
3. **选择（Selection）**：依据个体的适应度值选择个体参与后续的遗传操作。常见的选择方法有轮盘赌选择、锦标赛选择等。
4. **交叉（Crossover）**：模拟生物遗传学中的基因重组过程，将两个父代个体的部分基因交换，以产生新的后代个体。
5. **变异（Mutation）**：模拟生物遗传学中的基因突变过程，以一定的概率改变个体基因中的某些位，增加种群多样性。
6. **替换（Replacement）**：决定哪些个体将被新产生的后代所取代。

### 遗传算法的工作流程
1. **初始化种群**：随机生成一定数量的初始解作为种群。
2. **评估适应度**：使用适应度函数计算每个个体的适应度值。
3. **选择操作**：根据适应度值选择个体作为父母。
4. **遗传操作**：
    - 交叉：对选中的父母个体执行交叉操作，产生新的后代。
    - 变异：对后代个体执行变异操作。
5. **更新种群**：用新产生的后代替换旧的个体，形成新一代种群。
6. **循环迭代**：重复上述过程，直到满足停止条件（如达到最大迭代次数或适应度值不再显著提高等）。

### 优点
+ **全局搜索能力**：遗传算法能够有效地在解空间中进行全局搜索，不容易陷入局部最优。
+ **并行性**：遗传算法天然适合并行处理，因为可以同时处理多个个体。
+ **通用性强**：可以应用于多种类型的优化问题，包括离散、连续以及混合型问题。

### 缺点
+ **参数设置**：需要合理设置算法参数，如种群规模、交叉率、变异率等，这些参数的选择会影响算法性能。
+ **计算复杂度**：对于大规模问题，遗传算法的计算量可能较大。
+ **过早收敛**：如果参数设置不当，可能会导致种群过早收敛到局部最优解。

:::

    - **粒子群算法**

:::info
粒子群优化算法（Particle Swarm Optimization, PSO）是一种基于群体智能的优化算法，由Kennedy和Eberhart于1995年首次提出。PSO最初是受鸟群觅食行为启发而设计的，后来被广泛应用于各种优化问题，如函数优化、机器学习中的特征选择和权重优化、神经网络训练等。

### 基本原理
在粒子群算法中，每个潜在解被称为一个“粒子”，所有粒子组成了一个“群体”。每个粒子在多维搜索空间中具有一个位置（由一组坐标表示）和一个速度（决定了粒子如何移动）。粒子们通过跟踪两个极值来更新自己的位置：一个是粒子本身发现的最好位置（称为pbest，个人最佳），另一个是整个群体中所有粒子找到的最好位置（称为gbest，全局最佳）。

### 工作流程
1. **初始化**：创建一个由随机位置和速度组成的粒子群。
2. **评估适应度**：计算每个粒子的位置对应的适应度值。
3. **更新个人最佳（pbest）**：比较每个粒子当前的位置与它的历史最佳位置，如果当前位置更好，则更新其个人最佳位置。
4. **更新全局最佳（gbest）**：在群体中找到当前的最佳粒子位置。
5. **更新粒子的速度和位置**：根据一定的公式和策略调整粒子的速度，然后根据新的速度更新粒子的位置。
6. **迭代**：重复步骤2-5，直到满足终止条件（如达到最大迭代次数或适应度值达到某个阈值）。

### 更新规则
粒子的速度和位置更新遵循以下公式：

$v_i(t+1)=w⋅v_i(t)+c_1⋅r_1⋅(pbest_i−x_i(t))+c_2⋅r_2⋅(gbest−x_i(t))$

$x_i(t+1)=x_i(t)+v_i(t+1)$

其中，

+ vi(t)是粒子i在时间t的速度。
+ xi(t)是粒子i在时间t的位置。
+ w__是惯性权重，控制粒子保持原有运动趋势的程度。
+ c1和 c2是加速常数，分别影响粒子向个人最佳和全局最佳移动的力度。
+ r1和 r2是在[0,1]区间内均匀分布的随机数。

### 特点
+ **简单性**：粒子群算法的概念简单，易于理解和实现。
+ **并行性**：由于每个粒子的更新都是独立的，因此算法非常适合并行化。
+ **灵活性**：可以通过调整参数来适应不同的优化问题。

### 缺点
+ **容易过早收敛**：粒子群算法可能会快速收敛到局部最优解。
+ **参数敏感**：算法的效果很大程度上取决于参数的选取，如惯性权重w_w_，加速常数c1_c_1和c2_c_2等。

:::

 ## 评价决策类<
+ **层次分析法**

:::info
层次分析法（Analytic Hierarchy Process, AHP）是由美国运筹学家Thomas Saaty于20世纪70年代提出的一种多准则决策分析方法。AHP提供了一种系统化、数学化的方法来帮助决策者处理复杂的决策问题，尤其是在面对多个相互冲突的目标时。该方法通过将复杂问题分解为层次结构，并通过配对比较来量化不同准则的重要性，最终得出各备选方案的优先级。

### 层次分析法的基本步骤
1. **建立层次结构模型**：
    - 将决策问题分解为不同的层次，通常包括目标层、准则层和方案层。
        * **目标层**：定义决策问题的最终目标。
        * **准则层**：列出评估备选方案时考虑的不同标准或准则。
        * **方案层**：列出可供选择的具体方案或行动。
2. **构建判断矩阵**：
    - 对于每个层次上的元素（目标对准则、准则对方案），通过两两比较的方式来构建判断矩阵。
    - 比较尺度通常采用1-9标度，其中1表示两个要素同等重要，9表示一个要素极其重要。
    - 判断矩阵是一个n×n的方阵，其中n是同一层次中元素的数量，矩阵中的元素表示两两元素之间的相对重要性。
3. **计算权重向量**：
    - 对于每个判断矩阵，计算其最大特征值对应的特征向量，并对其进行归一化处理，得到各元素的权重。
    - 归一化的方法通常是将矩阵的每一行相加，然后将每个元素除以其所在行的总和。
4. **一致性检验**：
    - 由于人类判断可能存在不一致，需要对判断矩阵进行一致性比率（Consistency Ratio, CR）检验。
    - 计算一致性指标（Consistency Index, CI）：CI = (λ_max - n) / (n - 1)，其中λ_max是判断矩阵的最大特征值，n是矩阵的阶数。
    - 计算随机一致性指标（Random Consistency Index, RI）：RI是一个经验数值，对于不同阶数的矩阵有不同的RI值。
    - 计算一致性比率CR = CI / RI。
    - 如果CR < 0.1，则认为判断矩阵具有一致性，否则需要重新调整判断矩阵。
5. **合成权重**：
    - 将上层元素的权重与下层元素的权重相结合，得到最终的方案权重。
    - 对于每个方案，计算其在所有准则下的综合得分，综合得分最高的方案被认为是最佳选择。
6. **决策结果分析**：
    - 根据计算出的权重，分析各个方案的优先级，并据此做出决策。

### 层次分析法的优点
+ **系统性**：将复杂问题分解为层次结构，便于理解和分析。
+ **灵活性**：可以根据实际情况灵活调整层次结构和判断标准。
+ **透明性**：通过两两比较和一致性检验，使得决策过程更加透明和可信。

### 层次分析法的局限性
+ **主观性**：判断矩阵的构建依赖于专家的主观判断，可能因人而异。
+ **计算复杂性**：当层次结构复杂或方案数量较多时，计算量可能会很大。
+ **一致性检验**：一致性检验可能过于严格，有时即使CR略大于0.1，判断矩阵也可能具有一定的合理性。

:::

+ **TOPSIS 法**

:::info
TOPSIS（Technique for Order Preference by Similarity to Ideal Solution）法，即理想解排序法，是一种常用的多属性决策方法。该方法由Hwang和Yoon在1981年提出，旨在通过计算评价对象与理想解和负理想解的距离来进行排序，从而选出最优或最满意的方案。以下是TOPSIS法的详细介绍：

### 基本原理
TOPSIS法的基本思想是基于理想解的概念进行排序。理想解是一个设想的最优解，它的每个属性值都是所有备选方案中该属性的最优值；而负理想解则是一个设想的最劣解，它的每个属性值都是所有备选方案中该属性的最劣值。TOPSIS法通过计算每个方案与理想解和负理想解的距离，进而确定各方案的相对优劣。

### 主要步骤
1. **建立决策矩阵**：
    - 确定决策问题中的方案集和属性集。
    - 建立一个m×n的决策矩阵，其中m表示方案的数量，n表示属性的数量。
2. **矩阵归一化**：
    - 由于不同的属性可能有不同的量纲和量级，需要将决策矩阵进行归一化处理，使其转化为无量纲的标准化矩阵。
3. **确定权重**：
    - 根据属性的重要程度赋予不同的权重，权重向量通常表示为w = (w1, w2, …, wn)。
4. **构造加权决策矩阵**：
    - 将归一化矩阵与权重向量相乘，得到加权决策矩阵。
5. **确定理想解和负理想解**：
    - 理想解是由每个属性的最大值组成的向量，负理想解是由每个属性的最小值组成的向量。
6. **计算距离**：
    - 计算每个方案到理想解的距离Di*和到负理想解的距离Di-。
7. **计算相对接近度**：
    - 对于每个方案，计算其与理想解的相对接近度Ci，公式为Ci = Di- / (Di* + Di-)。
8. **排序和选择**：
    - 根据相对接近度Ci对方案进行排序，Ci越接近1，表示该方案越接近理想解，越优。

### 关键概念
+ **理想解**：一个假想的最佳方案，其所有属性值都是最优的。
+ **负理想解**：一个假想的最差方案，其所有属性值都是最差的。
+ **相对接近度**：衡量各方案接近理想解的程度。

### 优缺点
#### 优点
+ **简洁性**：方法简单，易于理解和操作。
+ **全面性**：考虑了所有属性的信息，减少了主观偏见的影响。
+ **灵活性**：适用于不同类型的数据和决策环境。

#### 缺点
+ **权重分配**：权重的主观确定可能影响结果。
+ **对极端值的敏感性**：如果数据中存在极端值，可能会对结果产生较大影响。

:::

+ **熵权法**

:::info
熵权法（Entropy Weight Method）是一种客观赋权方法，用于确定多个评价指标（或准则）的重要性权重。这种方法基于信息论中的熵概念，通过计算每个指标的信息熵来确定其权重。信息熵越高，表示该指标提供的信息量越少，反之则越多。因此，熵权法能够量化各个指标的信息贡献度，并据此赋予相应的权重。

### 熵权法的基本原理
熵权法的核心思想是，一个指标的熵值反映了该指标提供的信息量。如果一个指标的熵值较高，则意味着不同方案在这个指标上的表现差异不大，提供的信息较少；反之，如果熵值较低，则意味着不同方案在这个指标上的表现差异较大，提供的信息较多。因此，熵值越低的指标，其权重应当越大。

### 熵权法的步骤
1. **构建决策矩阵**：
    - 设有m个方案，n个评价指标，构建一个m×n的决策矩阵X，其中每个元素$x_{ij}$表示第i个方案在第j个指标下的评分。
2. **标准化决策矩阵**：
    - 由于不同指标可能有不同的量纲或量级，需要对决策矩阵进行标准化处理。
    - 对于效益型指标（越大越好），标准化公式为：$x_{ij}′=\frac{x_{ij}}{\sqrt{\sum_{i=1}^mx_{ij}^2}}$
    - 对于成本型指标（越小越好），标准化公式为：$x_{ij}'=\frac{1/x_{ij}}{\sqrt{\sum_{i=1}^m(1/x_{ij})^2}}$
3. **计算信息熵**：
    - 计算每个指标的信息熵e_j。$e_j=-\frac{1}{ln(m-1)}\sum_{i=1}^m(\frac{x_{ij}'}{\sum_{i=1}^m}ln\frac{x_{ij}'}{\sum_{i=1}^2x_{ij}'})$
    - 信息熵e_j的范围是[0, 1]，e_j越接近于1，表示该指标提供的信息越少。
3. **计算熵冗余**：
    - 计算每个指标的熵冗余d_j。$d_j=1-e_j$
    - 熵冗余d_j反映了该指标提供的有效信息量。
4. **计算熵权**：
    - 计算每个指标的熵权w_j。$w_j=\frac{d_j}{\sum_{j=1}^nd_j}$
    - 熵权w_j表示了各个指标的相对重要性。

:::

+ **灰色关联度分析**

 ## 数理统计< 
### 时间序列模型
+ **灰色预测**

:::info
灰色预测是一种数据分析方法，主要用于对含有不确定因素的系统进行预测。这种方法特别适用于那些数据量少且具有不完全性的系统。灰色预测的核心思想是通过对原始数据进行一定的生成处理（如累加生成），将其转化为具有较强规律性的数据序列，进而建立微分方程模型或其他数学模型来进行预测。

灰色预测模型由我国学者邓聚龙教授于20世纪80年代初提出，并逐渐发展成为灰色系统理论的一部分。灰色系统理论是一门研究少数据、贫信息不确定性问题的新学科，它与传统的概率统计方法相比，不需要大量的历史数据，因而被广泛应用于经济、管理、环境科学、工程等领域。

### 常见的灰色预测模型包括：
+ **GM(1,1)模型**：这是最基本的灰色预测模型，用来对单个变量的时间序列数据进行预测。它假设经过累加生成的数据序列符合指数增长规律，从而建立一阶微分方程模型来进行预测。
+ **灰色关联分析**：这是一种用于分析序列之间关联程度的方法，可以用来确定哪些因素对系统的影响更大。它通过计算序列间的“关联系数”和“关联度”，来判断不同序列之间的相似性或接近性。
+ **其他灰色预测模型**：还有诸如DGM(2,1)等扩展模型，以及结合灰色预测与其他方法（如神经网络、遗传算法等）的混合模型。

### 应用步骤：
1. 数据预处理：收集原始数据并对数据进行必要的生成处理，如累加生成（即逐项累加）等，以增强数据的规律性。
2. 建立模型：根据处理后的数据建立相应的灰色预测模型，比如GM(1,1)模型。
3. 参数估计：求解模型中的未知参数。
4. 模型检验：利用残差检验、关联度分析等方法来验证模型的有效性和准确性。
5. 预测输出：使用建立好的模型对未来数据进行预测。
6. 结果分析：对预测结果进行分析，并根据实际情况调整模型参数或选择更合适的模型。

灰色预测由于其对数据量要求不高，因此在数据获取困难或者样本量有限的情况下非常有用。但是，它的准确性和可靠性可能会受到数据本身特性和模型建立过程中假设条件的影响。

:::

### 计量统计
> 在合理的时间内找到一个足够好的解决方案，而不是保证找到全局最优解
>

+ **K-means 聚类(2017B)**

:::info
它的目标是将一组未标记的数据集分成K个簇（clusters），使得簇内的数据点彼此相似，而簇间的差异尽可能大。

以下是K-means聚类算法的基本步骤：

1. **初始化**：选择K个数据点作为初始的质心（centroid）。这通常可以通过随机选取或者使用一些启发式方法来完成。
2. **分配数据点**：将每个数据点分配给最近的质心所代表的簇。这里通常使用欧几里得距离度量数据点之间的相似性。
3. **更新质心**：重新计算每个簇的新质心。新质心是该簇内所有数据点坐标的平均值。
4. **重复步骤2和3**：重复上述过程，直到质心不再发生显著变化或达到预定的最大迭代次数为止。

K-means算法有一些优点：

+ 它简单且易于实现。
+ 当簇是密集的，并且簇与簇之间区别明显时，算法效率较高。

但是它也有一些缺点：

+ 必须提前确定K值，即簇的数量。
+ 对于非球形簇、不同大小和密度的簇，效果不佳。
+ 对异常值敏感，因为质心的位置会受到极端值的影响。
+ 收敛到局部最优解而不是全局最优解的可能性较大。

:::

+ **分层聚类**

:::info
分层聚类（Hierarchical Clustering）是一种聚类技术，它通过构建一个层次结构来对数据进行分类。这种层次结构可以表示为树状图（dendrogram），它展示了数据点是如何被逐步组合成更大的簇的过程。与K-means等其他聚类方法不同的是，分层聚类不需要预先指定簇的数量，而是可以根据需要从中提取不同数量的簇。

分层聚类有两种主要类型：凝聚型（Agglomerative）和分裂型（Divisive）。

### 凝聚型分层聚类
这是最常用的分层聚类形式。其基本思想是从单个数据点开始，逐步合并最相似的点或簇，形成更大的簇，直至所有点合并成一个簇。步骤如下：

1. **初始化**：每个数据点被看作是一个独立的簇。
2. **合并**：在每一步中，找到距离最近的两个簇并将其合并。这个过程重复进行，直到所有数据点被合并成一个簇。
3. **距离度量**：定义簇间距离的方法有多种，常见的有：
    - 单连接（Single Linkage）：两个簇间的距离定义为这两个簇中最接近的一对点的距离。
    - 完全连接（Complete Linkage）：两个簇间的距离定义为这两个簇中最远的一对点的距离。
    - 平均连接（Average Linkage）：两个簇间的距离定义为这两个簇中所有点对距离的平均值。
    - 重心法（Centroid Method）：两个簇间的距离定义为这两个簇质心之间的距离。

### 分裂型分层聚类
分裂型分层聚类则是从一个包含所有数据点的大簇开始，逐步将其分割成越来越小的簇。这种方法不如凝聚型常见，但在某些情况下可能是有用的。

### 分层聚类的优点和缺点
**优点**：

+ 不需要事先知道簇的数量。
+ 可以发现各种形状的簇，包括非凸形簇。
+ 结果可以通过树状图直观地展示出来，便于理解。

**缺点**：

+ 计算复杂度相对较高，特别是对于大数据集。
+ 对噪声和离群点敏感。
+ 一旦合并或分裂操作完成，就不能撤销，可能会导致局部最优解。

:::

+ **回归分析(2021B:乙醇偶合制备C4烯烃)**

:::info
回归分析是一种统计方法，用于研究变量之间的关系，尤其是因变量（也称为响应变量或输出变量）与一个或多个自变量（也称为解释变量或输入变量）之间的关系。回归分析的目标是建立一个模型，该模型可以用来预测或解释因变量的变化如何受自变量的影响。

### 基本概念
在回归分析中，我们通常假设因变量Y是自变量X的函数加上一些随机误差项。这个关系可以用以下方程来表示：

$Y=f(X)+ϵ$

其中：

+ Y__是因变量。
+ X__是自变量，可以是一个或多个。
+ f(X)__是自变量与因变量之间的关系函数。
+ ϵ__是随机误差项，反映了除了自变量之外影响因变量的因素。

### 主要类型的回归分析
1. **线性回归**：
    - **简单线性回归**：当只有一个自变量时，模型的形式为$Y=β_0+β_1X+ϵ$，其中$\beta_0$是截距，$β_1$是斜率。
    - **多元线性回归**：当有多个自变量时，模型的形式为$Y=β_0+β_1X_1+β_2X_2+…+β_nX_n+ϵ$。
2. **非线性回归**：
    - 当自变量与因变量之间的关系不是线性的时，可以使用非线性模型。这类模型的形式更加灵活，可以更好地拟合数据。
3. **逻辑回归**：
    - 虽然名字中有“回归”，但逻辑回归实际上是一种分类方法，用于预测事件发生的概率。逻辑回归模型通常用于二分类问题。
4. **多项式回归**：
    - 在某些情况下，自变量与因变量的关系可以通过多项式函数来描述，例如$Y=β_0+β_1X_1+β_2X_2+…+ϵ$。
5. **岭回归和Lasso回归**：
    - 这些方法在普通线性回归的基础上增加了正则化项，以防止过拟合。岭回归使用L2正则化，而Lasso回归使用L1正则化。
6. **其他回归方法**：
    - 时间序列回归、广义线性模型（GLM）、泊松回归、Cox比例风险回归等。

### 回归分析的步骤
1. **数据收集**：收集相关的数据。
2. **数据预处理**：处理缺失值、异常值等。
3. **模型选择**：根据问题的性质选择合适的回归模型。
4. **参数估计**：使用最小二乘法或其他方法来估计模型参数。
5. **模型评估**：使用残差分析、R²（决定系数）、AIC/BIC等指标来评估模型的好坏。
6. **模型验证**：通过交叉验证等方法来检验模型的泛化能力。
7. **预测**：使用模型来进行预测。

:::

+ **各类回归分析**

:::info
1. **线性回归 (Linear Regression)**
    - 最基础的回归模型之一，假设因变量与自变量之间存在线性关系。
    - 数学形式：$y=β_0+β_1x_1+β_2x_2+...+β_px_p+ϵ$
    - 其中，y 是因变量，$x_i$是自变量，$β_i$是回归系数，ϵ__ 是误差项。
2. **逻辑回归 (Logistic Regression)**
    - 适用于分类问题，特别是二分类问题。
    - 使用Sigmoid函数将线性组合转换为概率值。
    - 数学形式：$P(y=1∣x)=\frac{1}{1+e^{−(β_0+β_1x_1+...+β_px_p)}}$
3. **多项式回归 (Polynomial Regression)**
    - 当因变量与自变量之间的关系不是线性而是曲线关系时使用。
    - 包含自变量的幂次项，以捕捉更复杂的模式。
4. **逐步回归 (Stepwise Regression)**
    - 一种变量选择技术，自动地添加或删除自变量来改进模型。
    - 包括向前选择、向后消除和双向选择等方法。
5. **岭回归 (Ridge Regression)**
    - 当自变量之间存在多重共线性时使用。
    - 在损失函数中加入L2正则化项，以减小回归系数的幅度。
6. **套索回归 (Lasso Regression)**
    - 类似于岭回归，但在损失函数中加入L1正则化项。
    - L1正则化可以帮助进行变量选择，将一些不重要的变量的系数压缩至零。
7. **弹性网络回归 (Elastic Net Regression)**
    - 结合了L1和L2正则化的优势。
    - 适用于特征数量远大于样本数量的情况，同时能够执行稀疏解决方案。
8. **其他类型的回归模型**
    - 除了上述常见模型外，还有其他类型的回归模型，如广义线性模型（Generalized Linear Models）、泊松回归（Poisson Regression）、负二项回归（Negative Binomial Regression）、Cox比例风险回归（Cox Proportional Hazards Regression）等，它们针对不同类型的响应变量（如计数数据、生存数据等）和分布假设。

:::

> **多重共线性检验：**
>
> 是指在回归模型中，自变量之间存在较高的线性关系。这种现象可能会导致回归模型的估计不准确，参数解释困难，以及模型的预测能力下降。因此，在进行回归分析之前，进行多重共线性检验是非常必要的。
>

:::info
### 1. 多重共线性的影响
+ **参数估计不稳定**：自变量之间的共线性会导致回归系数的估计值对样本非常敏感，小的样本变化可能导致估计值的大幅波动。
+ **标准误增大**：共线性会导致回归系数的标准误被高估，从而降低参数估计的显著性。
+ **参数解释困难**：由于共线性，很难区分哪些自变量对因变量有重要影响。
+ **预测精度下降**：模型的预测能力会因为共线性而降低。

### 2. 多重共线性检验的方法
#### a. 相关系数检验
通过计算自变量之间的相关系数矩阵，可以初步判断是否存在多重共线性。如果相关系数的绝对值接近1，则可能存在共线性问题。

#### b. 容忍度（Tolerance）和方差膨胀因子（VIF）
+ **容忍度**：一个变量的容忍度定义为1减去该变量的方差分解比例（即R²），用于衡量多重共线性的程度。公式为：Tolerance = 1 - R²。如果容忍度很小，说明多重共线性问题严重。
+ **方差膨胀因子（VIF）**：VIF是容忍度的倒数，VIF = 1 / Tolerance。VIF值越大，多重共线性越严重。一般来说，VIF值大于10意味着严重的多重共线性。

#### c. 特征值和条件指数
+ **特征值**：通过计算自变量矩阵的协方差矩阵的特征值，如果发现有很多接近于零的特征值，则可能存在多重共线性。
+ **条件指数**：条件指数是最大特征值与最小特征值的比率的平方根。条件指数越大（一般大于30），共线性问题可能越严重。

### 3. 处理多重共线性的方法
+ **排除一些自变量**：如果某些变量之间存在高度相关，可以考虑排除其中一个。
+ **增加样本量**：更多的数据有时可以减轻共线性问题。
+ **合并变量**：将相关的变量合并为一个指标变量。
+ **岭回归（Ridge Regression）**：通过引入惩罚项来减少共线性的影响。
+ **主成分分析（PCA）**：通过提取主成分来降低变量间的相关性。

:::

![](https://cdn.nlark.com/yuque/0/2024/png/36047305/1725277248609-7958f4ec-8128-4023-bc45-71d8c481108f.png)

### 机器学习
+ **BP 神经网络**

:::info
BP神经网络（Backpropagation Neural Network）是一种多层前馈神经网络模型，它通过反向传播算法进行训练，以调整网络中的权重和偏置，从而使网络能够学习输入与输出之间的映射关系。BP神经网络是人工神经网络中最为经典和广泛应用的一种模型，其基本架构由输入层、一个或多个隐藏层和输出层组成。

### BP神经网络的基本结构
1. **输入层**：接收外部输入信号，通常每个输入节点对应输入数据的一个特征。
2. **隐藏层**：位于输入层和输出层之间，负责提取输入数据的特征。隐藏层可以有一个或多个，每个隐藏层由多个神经元组成。
3. **输出层**：产生网络的最终输出，输出层的神经元数量取决于任务的需求（如分类任务中类别数）。

每个神经元都有一个激活函数（如sigmoid函数、ReLU函数等），用于将加权求和的结果转换为神经元的输出。

### BP神经网络的工作原理
BP神经网络的学习过程分为两个主要阶段：前向传播（Forward Propagation）和反向传播（Backward Propagation）。

1. **前向传播**：
    - 输入层接收输入信号。
    - 信号从前向后依次通过每一层的神经元。
    - 每个神经元对其输入进行加权求和，并通过激活函数计算输出。
    - 输出层产生最终的预测输出。
2. **反向传播**：
    - 计算输出层的实际输出与期望输出之间的误差。
    - 误差通过网络反向传播，使用梯度下降法或其他优化算法来更新网络中的权重和偏置。
    - 更新权重时，需要计算每个权重的梯度（即误差对权重的偏导数），并通过学习率控制更新的幅度。

### 损失函数与优化算法
1. **损失函数**：衡量网络输出与真实标签之间的差异。常见的损失函数包括均方误差（Mean Squared Error, MSE）用于回归问题，交叉熵损失（Cross-Entropy Loss）用于分类问题。
2. **优化算法**：用于最小化损失函数，常见的优化算法有：
    - **梯度下降**（Gradient Descent）：通过计算损失函数关于权重的梯度，并沿负梯度方向更新权重。
    - **随机梯度下降**（Stochastic Gradient Descent, SGD）：每次只使用一个样本来更新权重，以加速训练过程。
    - **批量梯度下降**（Batch Gradient Descent）：使用整个数据集来更新权重，计算更精确但速度较慢。
    - **小批量梯度下降**（Mini-batch Gradient Descent）：折中方案，每次使用一小批样本来更新权重。

### 训练过程
1. **初始化权重**：随机初始化网络中的权重和偏置。
2. **前向传播**：计算网络的输出。
3. **计算损失**：根据损失函数计算当前权重下的损失值。
4. **反向传播**：计算损失关于权重的梯度，并更新权重。
5. **迭代训练**：重复前向传播和反向传播过程，直到满足停止条件（如达到最大迭代次数或损失值低于某个阈值）。

### 应用领域
BP神经网络因其强大的学习能力和泛化能力，在众多领域得到了广泛的应用，包括但不限于：

+ **图像识别**：通过卷积神经网络（CNN）识别图像中的物体。
+ **语音识别**：通过循环神经网络（RNN）或长短时记忆网络（LSTM）进行语音信号的处理和识别。
+ **自然语言处理**：文本分类、情感分析等任务。
+ **推荐系统**：通过学习用户的偏好来推荐个性化内容。
+ **游戏AI**：在游戏中模拟玩家的行为或优化策略。

### 优缺点
**优点**：

+ 能够处理非线性问题。
+ 具有较强的泛化能力。
+ 可以学习复杂的输入输出映射关系。

**缺点**：

+ 容易陷入局部最优。
+ 训练过程可能需要较长的时间。
+ 需要大量数据来训练模型。
+ 模型的解释性较差。

:::

+ 

### 相关分析
+ Person 相关分析
+ Spearman 相关性分析

:::info
斯皮尔曼等级相关系数（Spearman's rank correlation coefficient，也称为斯皮尔曼ρ或Spearman's ρ）是一种衡量两个变量之间单调关系强度和方向的统计量。与皮尔逊相关系数不同，斯皮尔曼相关系数基于数据的等级而不是实际数值，这使得它成为一种非参数统计方法，适用于不满足正态分布假定的数据，或者当数据是以等级形式给出的时候。

### 斯皮尔曼等级相关系数的特点：
+ **非参数性**：这意味着斯皮尔曼相关性分析不要求数据遵循特定的分布，比如正态分布。
+ **单调关系**：斯皮尔曼相关系数能够检测变量间是否存在单调增加或减少的关系，而不仅仅是线性关系。
+ **等级数据适用**：当数据只能以等级形式获得时，斯皮尔曼相关系数是一个很好的选择。

### 计算方法：
斯皮尔曼等级相关系数通常用符号 _ρ_ 或 _rs_ 来表示。对于给定的两组数据，首先将每组数据内的观察值按大小排序，得到各自的等级。然后，计算每个观测值等级之间的差值的平方和，根据这个差值的平方和，可以计算出斯皮尔尔曼等级相关系数：

$\rho=1-\frac{6\sum{d_i^2}}{n(n^2-1)}$

其中 _di_ 是第 _i_ 对观测值等级之间的差值，_n_ 是观测值的数量。

### 解释系数值：
+ 系数范围从 -1 到 +1。
+ 当 _ρ_=+1 时，表示两个变量有完全一致的单调递增关系；
+ 当 _ρ_=−1 时，表示两个变量有完全一致的单调递减关系；
+ 当 _ρ_ 接近于 0 时，表示变量间的关系较弱或不存在单调关系。

### 使用场景：
+ 在社会科学、心理学、医学研究等领域中，当研究者想要评估两个有序变量之间的关系时。
+ 当数据包含异常值或分布偏斜时，使用斯皮尔曼相关系数可能比皮尔逊相关系数更合适。
+ 在金融分析中，为了评估两个股票价格变动趋势的一致性时。

:::

 ## 其他算法< 
+ **二分法(2016A)**

:::info
二分法（Binary Search）是一种在有序数组中查找特定元素的高效算法。它的基本思想是通过不断将搜索区间减半来缩小搜索范围，从而快速定位目标元素。二分法通常用于查找和排序相关的任务中，尤其是在数据量较大时，其效率优势尤为明显。

### 二分法的基本步骤
1. **初始化**：设置两个指针 `low` 和 `high`，分别指向数组的起始位置和结束位置。
2. **中间位置**：计算中间位置 `mid`，通常是 `(low + high) / 2` 或者使用 `low + (high - low) / 2` 防止整数溢出。
3. **比较**：比较中间位置的元素 `arr[mid]` 与目标值 `target`。
    - 如果 `arr[mid]` 等于 `target`，则找到了目标值，返回 `mid`。
    - 如果 `arr[mid]` 小于 `target`，则将 `low` 设置为 `mid + 1`，继续在右半部分搜索。
    - 如果 `arr[mid]` 大于 `target`，则将 `high` 设置为 `mid - 1`，继续在左半部分搜索。
4. **重复步骤2和3**：直到 `low` 大于 `high`，此时表示目标值不存在于数组中。

二分法不仅用于查找元素，还可以用于解决其他类型的问题，例如：

+ **查找第一个或最后一个出现的元素**：在重复元素的数组中，可以修改二分法来找到目标元素第一次或最后一次出现的位置。
+ **查找最接近的元素**：如果目标元素不存在，可以找到与目标最接近的元素。
+ **数值逼近**：在某些数值求解问题中，可以使用二分法来逼近解，例如求平方根或解方程。
+ **优化问题**：在某些情况下，二分法可以用来优化目标函数，特别是在函数单调的情况下。

:::

+ **NASH均衡**

:::info
纳什均衡（Nash Equilibrium）是博弈论中的一个重要概念，由约翰·纳什（John Nash）在1950年代提出。纳什均衡描述了在多参与者决策问题中的一种稳定状态，在这种状态下，任何一方单方面改变策略都不会得到更好的结果。换句话说，每个参与者的策略都是对其他玩家策略的最佳反应。

### 纳什均衡的基本定义
在一个博弈中，如果每个参与者都选择了某一策略，并且没有参与者可以通过单方面改变自己的策略来获得更高的收益，那么这一组策略就构成了一个纳什均衡。

### 纳什均衡的特点
1. **稳定性**：在纳什均衡中，每个参与者的行为是最优的，因为他们无法通过单方面改变策略来增加自己的收益。
2. **存在性**：对于有限策略空间的博弈，至少存在一个纳什均衡（可能是在混合策略下）。
3. **可能不是最优解**：纳什均衡并不总是全局最优解或社会最优解。有时候，即使在纳什均衡下，所有参与者也可以通过合作达成更好的结果。

### 如何寻找纳什均衡
寻找纳什均衡通常涉及以下几个步骤：

1. **定义参与者和策略集**：明确博弈中的参与者以及每个参与者可以选择的所有可能策略。
2. **定义收益函数**：对于每个参与者，在给定的策略组合下，确定他们的收益。
3. **检查所有策略组合**：检查所有可能的策略组合，找出那些满足纳什均衡条件的组合。

### 纳什均衡的例子
#### 简单例子：囚徒困境
囚徒困境是一个经典的博弈论例子，可以用来说明纳什均衡的概念。假设有两个犯罪嫌疑人被捕，警方分别审讯他们。每个人有两个选择：沉默（不招供）或背叛（招供）。

收益矩阵如下：

:::

| | **对方沉默** | **对方背叛** |
| --- | --- | --- |
| 沉默 | (3, 3) | (0, 5) |
| 背叛 | (5, 0) | (1, 1) |


:::info
在这个矩阵中，数字对的第一个数字表示第一个嫌疑人的收益，第二个数字表示第二个嫌疑人的收益。收益越高，对嫌疑人越有利。

在这个例子中，(背叛, 背叛) 组合是一个纳什均衡，因为无论另一个嫌疑人做什么，背叛总是比沉默更好。虽然(沉默, 沉默) 对双方来说总收益更高，但它不是一个纳什均衡，因为任何一方都可以通过背叛来获得更好的结果。

:::

+ **Logit 回归**

:::info
Logit回归，也称为逻辑回归（Logistic Regression），是一种广泛使用的统计方法，用于解决分类问题，尤其是二分类问题。尽管名称中含有“回归”一词，但它实际上是一种分类模型，用于预测某个事件发生的概率。Logit回归模型通过估计一个事件发生的概率来预测分类结果，并且能够提供解释变量对响应变量影响的大小和方向。

### Logit回归的基本原理
#### 概率表示
在Logit回归中，我们关注的是二分类变量Y的概率模型，其中Y可以取0或1两个值（例如，0表示“不发生”，1表示“发生”）。

#### 模型形式
Logit回归模型的形式如下：$logit(p)=ln⁡(\frac{p}{1−p})=β_0+β_1x_1+β_2x_2+⋯+β_px_p$其中：

+ p表示事件发生的概率（即 P(Y=1∣X)）。
+ logit(p)称为对数几率（log odds），是对数几率比的简称。
+ β0,β1,β2,…,βp是模型参数。
+ x1,x2,…,xp是解释变量（自变量）。

#### Sigmoid函数
通过对数几率进行反变换，我们可以得到事件发生的概率 _p_：$p=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_px_p)}}$这个函数被称为Sigmoid函数或Logistic函数，其图形是一个S形曲线，范围在0到1之间，表示概率值。

### Logit回归的优点
1. **易于解释**：Logit回归的系数可以直接解释为对数几率的变化，且可以通过指数形式转换为几率比（odds ratio），这有助于理解解释变量对响应变量的影响。
2. **概率预测**：Logit回归不仅提供分类结果，还提供事件发生的概率预测。
3. **非线性关系**：Logit回归可以处理自变量与因变量之间的非线性关系。
4. **稳健性**：对于一些离群值，Logit回归模型较为稳健。

### Logit回归的应用
Logit回归广泛应用于各种领域，包括但不限于：

+ **市场营销**：预测客户是否会购买某个产品或服务。
+ **医疗健康**：预测患者是否会患上某种疾病。
+ **金融**：信用评分模型，预测贷款违约的可能性。
+ **社会科学**：研究社会现象的发生概率，如犯罪率、就业状况等。
+ **工程**：故障预测，判断设备是否会发生故障。

### Logit回归的实现步骤
1. **数据准备**：收集数据，并对数据进行预处理，如缺失值处理、异常值检测等。
2. **模型建立**：根据问题背景选择合适的解释变量，建立Logit回归模型。
3. **模型估计**：使用极大似然估计（Maximum Likelihood Estimation, MLE）方法来估计模型参数。
4. **模型诊断**：评估模型的拟合效果，包括模型显著性检验、变量显著性检验等。
5. **模型应用**：使用模型进行预测，并根据预测结果进行决策。

### 模型评估指标
1. **混淆矩阵**：用于评估分类模型的性能，包括真阳性（TP）、假阳性（FP）、真阴性（TN）、假阴性（FN）。
2. **准确率（Accuracy）**：正确分类的样本占总样本的比例。
3. **召回率（Recall）/灵敏度（Sensitivity）**：真正例被正确预测的比例。
4. **精确率（Precision）**：被预测为正例的样本中实际为正例的比例。
5. **F1分数**：召回率和精确率的调和平均值。
6. **ROC曲线和AUC值**：评估模型区分力的图形和数值指标。

### 限制与挑战
1. **线性假设**：虽然Logit回归模型可以处理自变量与因变量之间的非线性关系，但它假设自变量之间是线性相关的。
2. **过度拟合**：如果模型中包含过多的解释变量，可能导致过度拟合问题。
3. **独立同分布假设**：Logit回归假设每个观测值是独立的，并且误差项是同分布的。
4. **多重共线性**：当解释变量之间存在高度相关性时，可能会影响模型参数估计的稳定性。

:::

+ **最小二乘法**

:::info
最小二乘法（Least Squares Method）是一种数学优化技术，通常用于**曲线拟合和线性回归分析**中。这种方法的基本思想是找到一条**最佳拟合直线**（或曲线），使得所有数据点到这条直线（或曲线）的垂直距离平方和最小。

### 原理
对于一组数据点 (x1,y1),(x2,y2),…,(xn,yn)，我们希望找到一个函数 y=f(x)使得它能够很好地描述这些数据点之间的关系。在最简单的情况下，f(x)可以是一个一次函数 y=ax+b__。我们的目标是最小化误差平方和（Sum of the Squares of the Errors, SSE），即：

$SSE=∑_{i=1}^n(y_i−f(x_i))^2$

对于线性模型 y=ax+b，这意味着我们要找到系数 a 和 b__，使得上述方程达到最小值。

### 应用
最小二乘法广泛应用于统计学、经济学、工程学等领域中的数据建模。它可以用于：

+ **线性回归**：确定两个变量之间的线性关系。
+ **多项式拟合**：当数据点适合更高阶的多项式时，可以使用最小二乘法来找到最佳拟合的多项式。
+ **非线性拟合**：虽然名字中有“最小二乘”，但该方法也可以通过迭代的方式应用于非线性模型的参数估计。
+ **预测与决策制定**：基于已有的数据集建立模型后，可以用来预测未来的趋势或辅助决策过程。

### 计算方法
解决最小二乘问题的方法包括解析解（例如，对于线性回归可以通过矩阵运算直接求解）和数值解（如梯度下降等迭代算法）。对于线性回归问题，可以使用正规方程（Normal Equations）求解：$a=\frac{n\sum{x_iy_i}-\sum{x_i}\sum{y_i}}{n\sum{x_i^2}-(\sum{x_i})^2},
b=\frac{\sum{y_i}-a\sum{x_i}}{n}$

这里的a是斜率，而 b是截距。

最小二乘法不仅限于一维数据，还可以扩展到多维空间，处理多元线性回归等问题。

:::

+ **显著性检验**

:::info
显著性检验（Significance Testing）是在统计学中用于评估假设测试结果可靠性的方法。它的主要目的是决定观察到的数据是否提供了足够的证据来拒绝一个特定的假设（称为零假设），通常是为了支持另一个假设（称为备择假设）。

### 基本概念
+ **零假设 (H₀)**：通常是假设没有效果或差异的情况。例如，在比较两种药物的效果时，零假设可能是“这两种药物的效果相同”。
+ **备择假设 (H₁ 或 Hₐ)**：与零假设相反的假设，表示存在某种效果或差异。比如，“药物A比药物B更有效”。

### 过程
1. **设定假设**：明确零假设和备择假设。
2. **选择检验统计量**：根据问题类型选择合适的统计量，如t统计量、z统计量、卡方统计量等。
3. **确定显著性水平 (α)**：这是你愿意接受犯第一类错误的概率，即当零假设为真时拒绝零假设的概率。常见的显著性水平有0.05或0.01。
4. **计算检验统计量的值**：利用样本数据计算出实际的检验统计量值。
5. **比较检验统计量与临界值**：
    - **p值方法**：计算p值，即在零假设为真的情况下获得当前或更极端结果的概率。如果p值小于或等于显著性水平α，则拒绝零假设。
    - **临界区域方法**：如果检验统计量落在预先定义的拒绝区域内，则拒绝零假设。
6. **做出结论**：根据检验结果，决定是否拒绝零假设，并据此得出关于研究问题的结论。

### 常见的显著性检验
+ **t检验**：用于比较两个样本均值之间的差异，特别是当样本容量较小且总体方差未知时。
+ **z检验**：类似于t检验，但在样本量较大且总体标准差已知时使用。
+ **卡方检验**：用于检验分类数据的独立性或适配度。
+ **ANOVA (Analysis of Variance)**：用于比较三个或更多组别的平均数。
+ **非参数检验**：如曼-惠特尼U检验、克鲁斯卡尔-沃利斯检验等，适用于不满足正态分布或其他假设条件的数据。

### 注意事项
+ 显著性检验并不能证明零假设或备择假设为真，只能提供证据支持或反对零假设。
+ 显著性水平的选择应当基于研究设计和领域内的惯例。
+ 检验的统计功效（Power）也是需要考虑的因素，特别是当样本量不足时可能会导致检验无法检测到实际存在的效应。
+ 进行多重检验时，需要调整显著性水平以控制错误发现率（False Discovery Rate, FDR）或家庭聪明错误率（Family-Wise Error Rate, FWER）。

:::

